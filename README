cl-mpi provides CFFI bindings for the Message Passing Interface (MPI). MPI is
typically used in High Performance Computing to utilize big parallel computers
with thousands of cores. It features minimal communication overhead with a
latency in the range of microseconds.

If you have any questions or suggestions, feel free to contact me.

cl-mpi has been tested with mpich2 and openmpi.

* Usage
  An MPI program must be launched with "mpirun" or "mpiexec". These commands
  spawn multiple processes depending on your system and commandline
  parameters. Each process is identical, except that it has a unique rank that
  can be queried with (MPI-COMM-RANK). The ranks are assigned from 0 to
  (- (MPI-COMM-SIZE) 1).

  A wide range of communication functions is avilable to transmit messages
  between different ranks.

* Testing
  To run the testsuite:
  mpirun -np 2 sbcl --noinform --quit --eval "(asdf:test-system :cl-mpi)" --eval "(mpi:mpi-finalize)"
  mpirun -np 2 ccl -e "(asdf:test-system :cl-mpi) (mpi:mpi-finalize)"

  To run the benchmarks
  mpirun -np 2 sbcl --noinform --quit --eval "(asdf:test-system :mpi-benchmarks)" --eval "(mpi:mpi-finalize)"

* Authors
  Alex Fukunaga
  Marco Heisig

* Special Thanks
  This project was partially funded by the KONWIHR (The Bavarian Competence
  Network for Technical and Scientific High Performance Computing).

  Another big thanks to Nicolas Neuss for all the useful suggestions.

* TODOs
  - parallel IO
  - one-sided MPI 3.0 communications
  - full MPI benchmark suite
  - MPI extensions
  - collective operations
  - extensive testing
  - convenient communication layers on top of MPI
