cl-mpi provides CFFI bindings for the Message Passing Interface (MPI). MPI is
typically used in High Performance Computing to utilize big parallel computers
with thousands of cores. It features minimal communication overhead with a
latency in the range of microseconds.

If you have any questions or suggestions, feel free to contact me.

cl-mpi has been tested with mpich2 and openmpi.

* Usage
  An MPI program must be launched with "mpirun" or "mpiexec". These commands
  spawn multiple processes depending on your system and commandline
  parameters. Each process is identical, except that it has a unique rank that
  can be queried with (MPI-COMM-RANK). The ranks are assigned from 0 to
  (- (MPI-COMM-SIZE) 1).

  A wide range of communication functions is avilable to transmit messages
  between different ranks.

  In case you change the MPI implementation of your system, cl-mpi needs to be
  rebuilt. To do this, simply run (asdf:load-system :cl-mpi :force t).

  Often it is necessary to perform a serial asdf:load-system before the MPI
  parallel ones, because asdf fails if operations are executed concurrently
  from different lisp images on the same node.

* Testing
  To run the testsuite:
  mpirun -np 2 sbcl --noinform --quit --eval "(asdf:test-system :cl-mpi)" --eval "(mpi:mpi-finalize)"
  mpirun -np 2 ccl -b -e "(asdf:test-system :cl-mpi) (mpi:mpi-finalize)"

  To run the benchmarks
  mpirun -np 2 sbcl --noinform --quit --eval "(asdf:test-system :mpi-benchmarks)" --eval "(mpi:mpi-finalize)"
  mpirun -np 2 ccl -b -e "(asdf:test-system :mpi-benchmarks) (mpi:mpi-finalize)"

* Authors
  Alex Fukunaga
  Marco Heisig

* Special Thanks
  This project was funded by KONWIHR (The Bavarian Competence Network for
  Technical and Scientific High Performance Computing) and the Chair for
  Applied Mathematics 3 of Prof. Dr. Bänsch at the FAU Erlangen-Nürnberg.

  Big thanks to Nicolas Neuss for all the useful suggestions.

* TODOs (roughly in orde of priority)
** one-sided MPI 3.0 communications
   - memory windows
   - get/put operations
** more collective operations
   - allgatherv
   - allreduce
   - alltoall
   - alltoallw
   - allgatherv
   - scatter / scatterv
   - reduce
   - scan
** MPI IO
   - make MPI IO play nicely with lisp streams, pathnames, etc.
** full MPI benchmark suite
   - pingping
   - sendrecv
   - exchange
   - allreduce
   - reduce
   - reduce_scatter
   - allgather/allgatherv
   - gather
   - scatter / scatterv
   - alltoall / alltoallv
   - barrier
   - ...-ANYTHING MPI extensions
** MPI extensions
   - coarrays
   - checkpointing
   - allow different serialization tools, like cl-store
** testsuite improvements
   - one test per function
   - failure of individual ranks should be propagated to all ranks so that it
     is enough to see whether rank zero passes the testsuite
   - at least one real application test, e.g. a finite-difference solver
** datatype handling
   - model the memory of Lisp arrays via MPI_Datatypes
